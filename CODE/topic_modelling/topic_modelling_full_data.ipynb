{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9179bacf-bc6e-4c76-be00-e389d68fc2f2",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84296ee4-106e-4f1a-9b6a-33a048b0911e",
   "metadata": {
    "gather": {
     "logged": 1745019152024
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.20 (default, Oct  3 2024, 15:24:27) \n",
      "[GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d258c3-8a9c-4254-87c5-529f8460fc41",
   "metadata": {
    "gather": {
     "logged": 1744994932039
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/clean38/lib/python3.8/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/anaconda/envs/clean38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import all modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import regex as re\n",
    "import random\n",
    "import spacy\n",
    "import hdbscan\n",
    "import pickle\n",
    "import dill\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.decomposition import NMF\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from names_dataset import NameDataset\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from IPython.core.magic import register_cell_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3ae87-d232-4813-9028-30ad4066e6a9",
   "metadata": {
    "gather": {
     "logged": 1744994932812
    }
   },
   "outputs": [],
   "source": [
    "# # uncomment if not already donwloaded\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e48f6-d985-4fbc-ac6b-e3cd4612d5e5",
   "metadata": {
    "gather": {
     "logged": 1744994933430
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed \n",
    "seed = 1\n",
    "\n",
    "# Set global random seed  \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Note: local random seed set below in\n",
    "# - sampling\n",
    "# - LDA\n",
    "# - UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bcdaf9-b264-405b-afa8-a42320404398",
   "metadata": {
    "gather": {
     "logged": 1744994934038
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    '''\n",
    "    Skip the cell\n",
    "    '''\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6eff7-03b1-43b8-9520-fe4281574a23",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3b787-491d-4504-a37a-20a6e269315d",
   "metadata": {},
   "source": [
    "## Load and transform data (JSON to pandas df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42366861-bb97-4b5d-af2a-7bc450b05390",
   "metadata": {},
   "source": [
    "Note: Currently, we use for each earnings call the full transcripts. Alternatively, we could use for each earnings call the transcript_splits (statements by individual speakers). The latter would require changes below under \"flatten nested data structure\"; the resulting df should be kept equivalent to the current version with the exception of having more rows (transcript_splits instead of full transcripts) to work with subsequent code. However, unlike in the current version, adjustments to accomodate differences in interpretations (topic per earnigns call transcript vs topic per transcript_split/ speaker) might be necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8568db9-dc21-4c6a-bba7-697f7d525cfa",
   "metadata": {
    "gather": {
     "logged": 1744994965129
    }
   },
   "outputs": [],
   "source": [
    "# load data from JSON files\n",
    "path = './'\n",
    "file = 'full_combined.json'\n",
    "\n",
    "with open(path + file, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda47a80-b9e8-41df-9ea5-753a8601cac6",
   "metadata": {
    "gather": {
     "logged": 1744994965853
    }
   },
   "outputs": [],
   "source": [
    "# flatten nested data structure\n",
    "records = []\n",
    "for year in data.keys(): \n",
    "    for quarter in data[year].keys():\n",
    "        for report in data[year][quarter]:\n",
    "            records.append({\n",
    "                'company name': report['company name'],\n",
    "                'ticker': report['ticker'],\n",
    "                'sector': report['sector'],\n",
    "                'industry': report['industry'],\n",
    "                'year': year,\n",
    "                'quarter': quarter,\n",
    "                'date': report['transcript']['date'],\n",
    "                'text': report['transcript']['transcript']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9cabc-7bb6-4f1c-a429-aac1c8a433b0",
   "metadata": {
    "gather": {
     "logged": 1744994968502
    }
   },
   "outputs": [],
   "source": [
    "# create df\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad18f04-5376-4472-8106-a63d1f53c341",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab87f2c-9d49-4167-a56b-7aaf1e7e4e25",
   "metadata": {
    "gather": {
     "logged": 1744994969287
    }
   },
   "outputs": [],
   "source": [
    "# View first entries\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e145e-c7f9-44ac-a892-c9b322cca190",
   "metadata": {
    "gather": {
     "logged": 1744994970089
    }
   },
   "outputs": [],
   "source": [
    "# View last entries\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b00977-8552-4da2-852c-690f9ac136e2",
   "metadata": {
    "gather": {
     "logged": 1744994972758
    }
   },
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad9b4b-b101-4257-af4d-a50602375ccd",
   "metadata": {
    "gather": {
     "logged": 1744994973582
    }
   },
   "outputs": [],
   "source": [
    "# Check the info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd7de3-633c-4efb-b641-abf3a5829ba1",
   "metadata": {
    "gather": {
     "logged": 1744994974387
    }
   },
   "outputs": [],
   "source": [
    "# Check the descriptive statistics\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e9218-0b1d-4230-8021-20ef2312c244",
   "metadata": {
    "gather": {
     "logged": 1744994977437
    }
   },
   "outputs": [],
   "source": [
    "# print part of an example transcript\n",
    "print(df.iloc[0, ][\"text\"][0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0ced8-9cd7-46f3-aa1b-a8bae386fdc7",
   "metadata": {
    "gather": {
     "logged": 1744994978300
    }
   },
   "outputs": [],
   "source": [
    "# sector distribution\n",
    "df[\"sector\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a94af4-7519-4b2b-bb68-a898b0260a60",
   "metadata": {
    "gather": {
     "logged": 1744994979025
    }
   },
   "outputs": [],
   "source": [
    "# year distribution\n",
    "df[\"year\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93072147-c40b-409f-96da-801bc1f1c2b8",
   "metadata": {},
   "source": [
    "## Optionally: Choose a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf40e6-ab1b-43c8-8da3-65667760a28a",
   "metadata": {
    "gather": {
     "logged": 1744994981594
    }
   },
   "outputs": [],
   "source": [
    "# # uncomment for the final analysis\n",
    "# # select a sample for development purposes\n",
    "# sample_size = 2000\n",
    "# df = df.sample(sample_size, random_state=seed)\n",
    "\n",
    "# # check the shape\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f09556-106f-4d1a-b8a0-b93f924cf27c",
   "metadata": {},
   "source": [
    "# Topic modelling: Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7a4b0-e10a-4451-9acd-1bae64ff17ab",
   "metadata": {},
   "source": [
    "Note: Preprocessing includes (1) cleaning (with sentence- and word-level tokenization) and (2) feature extraction (creating a numerical representation of the text, i.e. a document-term matrix or DTM for short). NMF and LDA require both but differ in the best feature engineering; BERTopic internalizes feature engineering (no DTM as input needed) and can handle uncleaned and cleaned text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8550649-9422-42cb-8f33-c9a22d64f79f",
   "metadata": {},
   "source": [
    "## Preprocessing 1: Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d047e2f-78ee-415a-aed8-78b80e0ca417",
   "metadata": {},
   "source": [
    "#### Replace financial numbers with the word \"fin_num\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67158e32-63cc-4b3c-adcd-b60b94aa1812",
   "metadata": {},
   "source": [
    "Note: Financial numbers are replaced with \"fin_num\" to normalize numeric expressions, reduce noise, and retain meaningful signals for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df6921-a03e-4ea0-9464-78299a7d01f9",
   "metadata": {
    "gather": {
     "logged": 1744994982363
    }
   },
   "outputs": [],
   "source": [
    "# define function\n",
    "def substitute_financial_numbers(string):\n",
    "    '''\n",
    "    Substitues financial numbers by \"fin_num\" in a string\n",
    "    '''\n",
    "    sub_string = re.sub(\n",
    "        r\"\\$\\s?[0-9.,']+(?:\\s?(?:million|billion|thousand))?|[0-9.,']+%\",\n",
    "        \" fin_num \", \n",
    "        string) \n",
    "    return sub_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9250d-ee31-4775-8bb1-ed54f7e38fae",
   "metadata": {
    "gather": {
     "logged": 1744994985471
    }
   },
   "outputs": [],
   "source": [
    "# Apply function to df\n",
    "df[\"text_clean\"] = df[\"text\"].apply(substitute_financial_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787d28a-0ae2-47e4-b3f0-70a480fbb22e",
   "metadata": {},
   "source": [
    "#### Filter words and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ea1b1-319c-40b3-ba40-fec12363b693",
   "metadata": {},
   "source": [
    "Note: We require words to contain only alphabetic characters, be at least three characters long, and to be not first names (F); exceptions are \"AI\", \"US\", \"fin_num\". In addition, we require sentences to contain fewer than 50% occurrences of the word \"fin_num\" and to be at least five words long. (F) Footnote: First results showed that the DTM and the resulting topic-word-matrices contain many first names; filtering by NER is too slow and only somewhat effective; therefore, we choose a simpler and more effective approach here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106e8dd-a467-4c8f-bc0a-65d41c25a734",
   "metadata": {
    "gather": {
     "logged": 1744994986427
    }
   },
   "outputs": [],
   "source": [
    "# define function\n",
    "def filter_words(string, common_names, word_length=3, exceptions=None):\n",
    "    '''\n",
    "    Filters words in a string\n",
    "    - Req. 1: word is alpha (excl. numbers and special characters)\n",
    "    - Req. 2: word has a certain length (default: 3 characters) \n",
    "    - Req. 3: word is not a first name\n",
    "    - Exceptions: list of words exempt from the req (e.g., \"AI\") (default: None)\n",
    "    '''\n",
    "    if exceptions is None:\n",
    "        exceptions = []\n",
    "    keep_words = []\n",
    "    for word in word_tokenize(string):\n",
    "        if ((word.isalpha() \n",
    "             and len(word) >= word_length \n",
    "             and word.lower() not in common_names) \n",
    "            or word in exceptions):\n",
    "            keep_words.append(word)\n",
    "    return \" \".join(keep_words)\n",
    "\n",
    "# test the function\n",
    "test_string = '''Artificial intelligence, or short AI, boosts return \n",
    "fin_num by a factor of 10. However, Jim not 100%. Revenue fin_num fin_num fin_num.'''\n",
    "\n",
    "print(\"Test:\", filter_words(test_string, common_names=[\"Jim\"], exceptions=[\"AI\", \"fin_num\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a109c4-ed4f-48fb-a92d-689b75122e87",
   "metadata": {
    "gather": {
     "logged": 1744994987134
    }
   },
   "outputs": [],
   "source": [
    "# define function\n",
    "def filter_words_sentences(string, common_names, word_length=3, exceptions=None, \n",
    "                           sent_length=5, fin_num_th=0.5):\n",
    "    '''\n",
    "    Filters words (by call of filter_words) and sentences in a string\n",
    "    - Req. 1, 2, 3 and exceptions: see function filter_words\n",
    "    - Req. 4: sentence has certain length (default: 5 words)\n",
    "    - Req. 5: sentence has less than certain percentage of word \"fin_num\" \n",
    "      in it default: 0.5)\n",
    "    '''\n",
    "    keep_sents = []\n",
    "    for sent in sent_tokenize(string):\n",
    "        words = filter_words(sent, common_names, word_length, exceptions).split() # incl. word_tokenize\n",
    "        if (len(words) >= sent_length \n",
    "            and words.count(\"fin_num\")/len(words) <= fin_num_th):\n",
    "            keep_sents.append(\" \".join(words))\n",
    "    return \" \".join(keep_sents)\n",
    "\n",
    "# test the function:\n",
    "print(\"Test:\", filter_words_sentences(test_string, common_names=[\"Jim\"], exceptions=[\"AI\", \"fin_num\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823bd6f-5fc9-4e03-b2c7-f148a10ef485",
   "metadata": {
    "gather": {
     "logged": 1744996235442
    }
   },
   "outputs": [],
   "source": [
    "# Apply function to df\n",
    "\n",
    "# define common names (US)\n",
    "number_names = 1000\n",
    "nd = NameDataset()\n",
    "top_male = nd.get_top_names(n=number_names, gender='Male', country_alpha2='US')['US']['M']\n",
    "top_female = nd.get_top_names(n=number_names, gender='Female', country_alpha2='US')['US']['F']\n",
    "common_names = set(name.lower() for name in top_male + top_female)\n",
    "\n",
    "# define exceptions\n",
    "exceptions = [\"AI\", \"US\", \"fin_num\"]\n",
    "\n",
    "# call function\n",
    "df[\"text_clean\"] = df[\"text\"].apply(\n",
    "    filter_words_sentences,\n",
    "    word_length=3,\n",
    "    common_names=common_names,\n",
    "    exceptions=exceptions, \n",
    "    sent_length=5, \n",
    "    fin_num_th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d226a-07f7-4564-b49c-9be4f01212bb",
   "metadata": {
    "gather": {
     "logged": 1744996237942
    }
   },
   "outputs": [],
   "source": [
    "# View first entries\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2997b2da-abb0-43a4-9d2c-10ecde2570aa",
   "metadata": {
    "gather": {
     "logged": 1744996238995
    }
   },
   "outputs": [],
   "source": [
    "# print part of a cleaned example transcript\n",
    "print(df.iloc[0, ][\"text_clean\"][0:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e252070-4910-4668-90bb-13264bd278d5",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77104d42-f0d2-47f8-884a-300fd5f9ccd4",
   "metadata": {},
   "source": [
    "Note: We lemmatize words. Lemmatizing converts words to their base forms, reducing the inflectional variability in your texts. This is only hedlpful for LDA and NMF, as BERTopic can deal with this. Footnote: Earlier attempts showed that diffreent forms of the same word (e.g., singular and plural) showed up in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb4def-a5db-4b59-8574-8fdda5a4fc6b",
   "metadata": {
    "gather": {
     "logged": 1744996239516
    }
   },
   "outputs": [],
   "source": [
    "# Define function: \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_text(string):\n",
    "    \"\"\"\n",
    "    Lemmatizes the string\n",
    "    \"\"\"\n",
    "    doc = nlp(string)\n",
    "    return \" \".join(token.lemma_ for token in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1d30c-1373-48ca-bd98-be7af7c5a28d",
   "metadata": {
    "gather": {
     "logged": 1744996832408
    }
   },
   "outputs": [],
   "source": [
    "# Appply the function\n",
    "tqdm.pandas()\n",
    "df[\"text_clean_lemma\"] = df[\"text_clean\"].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36ba27-8ef5-46ae-a911-cbc1665cbf9e",
   "metadata": {
    "gather": {
     "logged": 1744996832505
    }
   },
   "outputs": [],
   "source": [
    "# View first entries\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa0b44-d609-49bb-a45b-49e6de31b3db",
   "metadata": {
    "gather": {
     "logged": 1744996832579
    }
   },
   "outputs": [],
   "source": [
    "# print part of a cleaned example transcript\n",
    "print(df.iloc[0, ][\"text_clean_lemma\"][0:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5cff6-3def-46ef-8ca2-2b1f6a837433",
   "metadata": {},
   "source": [
    "#### Optionally: Save or load preprocessed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c79044-27cf-4332-b51a-50dafd70cf28",
   "metadata": {
    "gather": {
     "logged": 1744996832644
    }
   },
   "outputs": [],
   "source": [
    "# To save:\n",
    "df.to_csv(\"./df.csv.gz\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e56958-3368-4901-955e-9de1c37bc6a7",
   "metadata": {
    "gather": {
     "logged": 1744996832791
    }
   },
   "outputs": [],
   "source": [
    "# # To load:\n",
    "# df = pd.read_csv(\"./df.csv.gz\")\n",
    "\n",
    "# # Check for NaNs introduced through saving: \n",
    "# problem_rows = df[~df[\"text_clean_lemma\"].apply(lambda x: isinstance(x, str))]\n",
    "# print(f\"Problem rows: {len(problem_rows)}\")\n",
    "# print(problem_rows.head())\n",
    "\n",
    "# # Drop NaN rows intriduced through saving:\n",
    "# df = df[df[\"text_clean_lemma\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae806a9-5809-494c-b6cc-8e21c22eab23",
   "metadata": {},
   "source": [
    "## Preprocessing 2: Feature extraction/ Document-term matrix (DTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db520e8-dd0e-4a14-b5f2-0258a2a42b11",
   "metadata": {},
   "source": [
    "Note: There are two options for DTMs: (1) DTM-TF, which includes simple word counts (term frequencies) for each document, and (2) DTM-TF-IDF, which weights terms by their inverse frequency in the corpus (term frequency–inverse document frequency). LDA requires DTM-TF, while NMF works best with DTM-TF-IDF. (BERTopic requires neither.) For both DTMs, we are case-insensitive (convert all text to lowercase), consider unigrams (single words) and bigrams (expressions consisting of two words), remove stopwords (i.e., common words that typically do not influence meaning), exclude words that appear in more than 50% of documents, and restrict the vocabulary to the 1,000 most frequent words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c597aa-07c4-4fdb-843e-1cab7989db6e",
   "metadata": {},
   "source": [
    "#### DTM-TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429449fc-686f-4f28-8c51-82c37c972125",
   "metadata": {
    "gather": {
     "logged": 1744996833498
    }
   },
   "outputs": [],
   "source": [
    "# load stop words\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Create DTM-TF\n",
    "start = time.time()\n",
    "\n",
    "vec_tf = CountVectorizer(\n",
    "    analyzer=\"word\", # Tokenize text at the word level\n",
    "    tokenizer=lambda x: x.split(), # simple split bc text pre-tokenized (see preprocess.)\n",
    "    token_pattern=None, # disable regex-based tokeniz. bc text pre-tokenized (see preprocess.)\n",
    "    lowercase=True, # convert tokens to lowercase\n",
    "    stop_words=list(stops), # exclude stop words\n",
    "    ngram_range=(1, 2), # allow unigrams and bigrams\n",
    "    max_df=0.5, # exclude tokens appearing in >50% of docs\n",
    "    max_features=1000) # limit vocabulart to 1000 most common tokens\n",
    "\n",
    "dtm_tf = vec_tf.fit_transform(df[\"text_clean_lemma\"])\n",
    "vocab_tf = vec_tf.get_feature_names_out()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"time elapsed (seconds): {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b0522-3454-4e24-8d62-b6db0bb5cb76",
   "metadata": {
    "gather": {
     "logged": 1744996834290
    }
   },
   "outputs": [],
   "source": [
    "# sanity check:\n",
    "display(dtm_tf.todense())\n",
    "dtm_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb164cfb-658a-4a10-be3a-31cbb0f1c5ee",
   "metadata": {},
   "source": [
    "#### DTM-TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e92fc-31b0-4374-9844-d8ad2129945f",
   "metadata": {
    "gather": {
     "logged": 1744996834501
    }
   },
   "outputs": [],
   "source": [
    "# load stop words\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Create DTM-TF\n",
    "start = time.time()\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\", # Tokenize text at the word level\n",
    "    tokenizer=lambda x: x.split(), # simple split bc text pre-tokenized (see preprocess.)\n",
    "    token_pattern=None, # disable regex-based tokeniz. bc text pre-tokenized (see preprocess.)\n",
    "    lowercase=True, # convert tokens to lowercase\n",
    "    stop_words=list(stops), # exclude stop words\n",
    "    ngram_range=(1, 2), # allow unigrams and bigrams\n",
    "    max_df=0.5, # exclude tokens appearing in >50% of docs\n",
    "    max_features=1000) # limit vocabulart to 1000 most common tokens\n",
    "\n",
    "dtm_tfidf = vec_tfidf.fit_transform(df[\"text_clean_lemma\"])\n",
    "vocab_tfidf = vec_tfidf.get_feature_names_out()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"time elapsed (seconds): {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb4f69-a880-4079-b388-0e396da3f177",
   "metadata": {
    "gather": {
     "logged": 1744996834650
    }
   },
   "outputs": [],
   "source": [
    "# sanity check:\n",
    "display(dtm_tfidf.todense())\n",
    "dtm_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d888c-a0ab-4c95-b1b2-700e50be901f",
   "metadata": {},
   "source": [
    "## LDA, NMF and BERTopic models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ab8e0-fb29-4a9c-92b6-f9b41a469e40",
   "metadata": {},
   "source": [
    "### Theoretical background "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07942d0a-c1df-4e86-bf85-6ef449a2de9e",
   "metadata": {},
   "source": [
    "#### LDA notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620541f7-4b98-43ed-9d34-a46db80d7c19",
   "metadata": {},
   "source": [
    "LDA notes:\n",
    "We use topic_word_prior = 0.01, doc_topic_prior = 50/n; these priors are not the default but often recommended in the literature; the model is stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f4a0a-ea86-4e16-9ac6-e861f487b192",
   "metadata": {},
   "source": [
    "Interpretation of output:\n",
    "- Document-topic matrix: Each row represents a document (e.g., a transcript of an earnings call), and each column represents a topic. The matrix can be interpreted as a topic distribution for each document, i.e., the proportion or probability that a document is associated with a given topic (for each row: sum over columns = 1).\n",
    "- Topic-word matrix: Each row corresponds to a topic, and each column corresponds to a word from the vocabulary in the DTM. Originally, the values (from model.components_) contain pseudocounts, i.e. estimated number of times word j assigned to topic i. After normalization (m_topic_word = m_doc_topic / m_doc_topic(axis=1, keepdims=True), the can be viewed as word distribution for each topic, i.e., the probability that a given word appears in a topic (for each row: sum over columns = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073131c-755f-4665-840f-dcf50581fdf4",
   "metadata": {},
   "source": [
    "#### NMF notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f80a6c-6867-433e-b7d1-fe23aa1f2172",
   "metadata": {},
   "source": [
    "NMF notes: \n",
    "The model is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09e47e-6f62-42ac-aeb6-0d01ab9a4a36",
   "metadata": {},
   "source": [
    "Interpretiation of the output: \n",
    "- Document-topic matrix: Each row represents a document (transcript of an earnings call), and each column represents a topic. Originally, the values (from model.fit_transform) reflect the strength or weight of each topic in the corresponding document — that is, how much the document loads onto each topic. After normalization (m_doc_topic / m_doc_topic.sum(axis=1, keepdims=True)), the matrix can be interpreted as a topic distribution for each document, i.e., the proportion or probability that a document is associated with a given topic (for each row: sum over columns = 1).\n",
    "\n",
    "- Topic-word matrix: Each row corresponds to a topic, and each column corresponds to a word from the vocabulary in the DTM. Originally, the values (from model.components_) represent the strength or weight of association between word j and topic i. After normalization (model.components_ / model.components_.sum(axis=1, keepdims=True)), the matrix can be interpreted as a word distribution for each topic, i.e., the probability that a given word appears in a topic (for each row: sum over columns = 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b724e-1c3e-4e6e-8985-f3659dd53521",
   "metadata": {},
   "source": [
    "#### BERTOpic notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a6cfc-f4d3-430c-abb7-dd583e13fd27",
   "metadata": {},
   "source": [
    "BERTopic notes:\n",
    "Default BERTopic incolves the following steps (source: GPT):\n",
    "\n",
    "1. Embedding Documents – Using a transformer model on each document to convert its raw text into dense vector representations that capture the semantic meaning \n",
    "2. Reducing Dimensionality – Use UMAP to compress the high-dimensional embeddings into a lower-dimensional space while preserving their structural relationships\n",
    "3. Clustering Reduced Embeddings into Topics – Apply HDBSCAN to group similar document embeddings, where each cluster represents a topic\n",
    "4. Tokenization of Topics – For each cluster, tokenize the original documents to extract words and phrases that occur frequently\n",
    "5. Weight Tokens – Use TF-IDF weighting to score and rank tokens by their importance within each topic\n",
    "6. Represent Topics – Summarize each topic by selecting the top-weighted tokens\n",
    "\n",
    "Clustering:\n",
    "\n",
    "\"BERTopic approaches topic modeling as a cluster task and attempts to cluster semantically similar documents to extract common topics. A disadvantage of using such a method is that [in the default version] each document is assigned to a single cluster and therefore also a single topic. In practice, however documents may conatin.\" (Grootendorst 2024) That is, BERTopic is more a topic clustering (assign 1 topic) than topic modelling approach (assign multiple topics).\n",
    "To assign multiple topics to documents there are several metods using BERTopic (Grootendorst 2024):\n",
    "1. Applying BERTopic on parts of the documents (e.g., sentences).\n",
    "2. Use a cluster model that can perform soft clustering like HDBSCAN.\n",
    "3. Use .approximate_distrbution: \"each document is split into tokens according to the provided tokenizer in the CountVectorizer. Then, a sliding window is applied on each document creating subsets of the document\".\n",
    "\n",
    "Test with the default version have shown: \n",
    "\n",
    "- Assigns only one topic to each document (see above for the reason \n",
    "- Applying the model on raw data leads to non-sensicel results.\n",
    "- The number of topics (around 20) and words per topic (around 20) are small. \n",
    "\n",
    "To use BERTopic in alingment with LDA and NMF, we therefore use a custoized BERTopic model:\n",
    "\n",
    "1. Multiple topics per document: Option 1: Use HDSBCAN (default), which does soft-clustering, and use calculate_probabilities=True (non-default). Option 2: Use k-means (non-default), which does hard clustering, and use .approximate_distrbution.\n",
    "2. Increase number of topics: In HDBSCAN (option 1) the number of clusters/ topics can not be set but indirectly controlled by min_cluster_size; the default is 15, so chose a lower number to increase the number of topics. In k-means we can directly set the number of clusters/ topics to 100. https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html#controlling-number-of-topics, https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html#k-means\n",
    "3. Increase the words per topic: Use CountVectorizer's parameter max_features to set the number of words per topic. \n",
    "4. Follow LDA and NMF as closely as possible in all other regards: Use the preprocessed data, excl. stopwords, lowercase, exclude to frequent words (max_df=0.5).\n",
    "5. Document-topic and topic-word matrices: Created in analogy to LDA and NMF: https://maartengr.github.io/BERTopic/faq.html#how-do-i-calculate-the-probabilities-of-all-topics-in-a-document, https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html#example, https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#topic-term-matrix, \n",
    "\n",
    "Randomness:\n",
    "\n",
    "BERTopic is a stochastic model. But its not easy to random seeds in the default version. Thus, we only do it for the final customized version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab8758-b20c-458d-acbc-eec4598b7cee",
   "metadata": {},
   "source": [
    "Interpretation of the output:\n",
    "\n",
    "- Document-Topic Matrix: Each row represents an earnings call transcript, and each column represents a topic. Originally, the values reflect the strength or weight (based on clustering and the underlying TF-IDF scores) of each topic for that document. After normalization, each row sums to 1, and you can interpret the entries as the probability or proportion of the document that is associated with each topic.\n",
    "\n",
    "- Topic-Word Matrix (BERTopic): Each row corresponds to a topic and each column to a word in the vocabulary. Initially, the values indicate the relative importance of a word within the topic (often derived from TF-IDF weights). Normalizing each row so that it sums to 1 lets you interpret the entries as the probability of a given word appearing in that topic.\n",
    "\n",
    "- Note that BERTopic reserves the label –1 for documents that are considered outliers and are not confidently assigned to any topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafccaf-b2d8-4a30-bafd-8e49bca4f361",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c97506-d37a-4735-a215-bcfa4b68fff1",
   "metadata": {
    "gather": {
     "logged": 1744996834788
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def normalize_matrix(df_matrix):\n",
    "    '''\n",
    "    Normalizes a document-topic or topic-word matrix, i.e. makes the row_sum = 1,\n",
    "    i.e. makes the document-topic matrix interpretable as a topic distribution for \n",
    "    each document and the the topic-word matrix as a word distribution for each topic\n",
    "    \n",
    "    Paramters:\n",
    "    - df_matrix: pd.DataFrame, non-normalized document-topic or topic-word matrix\n",
    "\n",
    "    Output:\n",
    "    - pd.DataFrame, normalized document-topic or topic-word matrix\n",
    "    '''\n",
    "    return df_matrix.div(df_matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4336576-04e7-453a-89fa-aff8021ddc71",
   "metadata": {
    "gather": {
     "logged": 1744996834947
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def get_topn_words(df_topic_word, number_top_words):\n",
    "    \"\"\"\n",
    "    Get top n words for each topic from topic-word matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - df_topic_word : pd.DataFrame, topic-word matrix \n",
    "    - number_top_words : int, number of top words to extract\n",
    "\n",
    "    Output:\n",
    "    - list of list: list containing top n words for each topic\n",
    "    \"\"\"\n",
    "    top_words_per_topic = []\n",
    "    for i in range(df_topic_word.shape[0]):\n",
    "        top_words = df_topic_word.iloc[i].nlargest(number_top_words).index.tolist()\n",
    "        top_words_per_topic.append(top_words)\n",
    "    return top_words_per_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f5e01-81e1-4a8d-9d64-09ae59c5dfca",
   "metadata": {},
   "source": [
    "### Functions for the models (incl. Parameterisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a5046-7afb-451e-ba47-e1037e06d9a7",
   "metadata": {},
   "source": [
    "#### LDA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71119a1-39bc-4935-80ad-79461eac0d54",
   "metadata": {
    "gather": {
     "logged": 1744996835182
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def lda_fun(dtm, vocab, number_topics, seed):\n",
    "    '''\n",
    "    Fits LDA model and returns model, document-topic and topic-word matrices\n",
    "    \n",
    "    Parameters:\n",
    "    - dtm: np.ndarray, document-term matrix\n",
    "    - number_topics: int, number of topics\n",
    "    - vocab: list of str, feature names/ dtm column names (i.e., the words)\n",
    "    - seed: int, random seed for reproducibility\n",
    "\n",
    "    Outputs:\n",
    "    - lda: fitted LDA model\n",
    "    - df_doc_topic: pd.DataFrame, document-topic distr. matrix (docs x topics)\n",
    "    - df_topic_word: pd.DataFrame, topic-word distr. matrix (topics x words)\n",
    "\n",
    "    Notes:\n",
    "    - output matrices are already normalized (for each row: sum over columns = 1)\n",
    "    '''\n",
    "    # define model\n",
    "    lda = LDA(\n",
    "        n_components=number_topics,\n",
    "        doc_topic_prior=min(50 / number_topics, 1),\n",
    "        topic_word_prior=0.01,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed)\n",
    "    \n",
    "    # Fit model and get document-topic-matrix\n",
    "    m_doc_topic = lda.fit_transform(dtm)\n",
    "    \n",
    "    # Get topic-word-matrix\n",
    "    m_topic_word = lda.components_\n",
    "\n",
    "    # Transform document-topic-matrix and topic-word-matrix to df\n",
    "    df_doc_topic = pd.DataFrame(\n",
    "        m_doc_topic, \n",
    "        columns=[f\"topic_{i}\" for i in range(lda.n_components)])\n",
    "\n",
    "    df_topic_word = pd.DataFrame(\n",
    "        m_topic_word, \n",
    "        columns=vocab)\n",
    "    \n",
    "    return {\"model\": lda, \n",
    "            \"df_doc_topic\": df_doc_topic, \n",
    "            \"df_topic_word\": df_topic_word}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4cf25-8867-4973-9832-04c05aaf5ce3",
   "metadata": {},
   "source": [
    "#### NMF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03717e19-425b-4222-ab2c-49b31db0ec9c",
   "metadata": {
    "gather": {
     "logged": 1744996835308
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def nmf_fun(dtm, vocab, number_topics):\n",
    "    '''\n",
    "    Fits a NMF model and returns model, document-topic and topic-word matrices\n",
    "    \n",
    "    Parameters:\n",
    "    - dtm: np.ndarray, document-term matrix\n",
    "    - number_topics: int, number of topics\n",
    "    - vocab: list of str, feature names/ dtm column names (i.e., the words)\n",
    "\n",
    "    Outputs:\n",
    "    - nmf: fitted LDA model\n",
    "    - df_doc_topic: pd.DataFrame, ocument-topic distr. matrix (docs x topics)\n",
    "    - df_topic_word: pd.DataFrame, topic-word distr. matrix (topics x words)\n",
    "\n",
    "    Notes:\n",
    "    - output matrices are not yet normalized (row sums != 1)\n",
    "    - nmf is deterinistic, i.e. reproducable without seed\n",
    "    '''\n",
    "    # Define model\n",
    "    nmf = NMF(\n",
    "        n_components=number_topics)\n",
    "    \n",
    "    # Fit model and get document-topic-matrix\n",
    "    m_doc_topic = nmf.fit_transform(dtm)\n",
    "    \n",
    "    # Get topic-word-matrix\n",
    "    m_topic_word = nmf.components_\n",
    "\n",
    "    # Transform document-topic-matrix and topic-word-matrix to df\n",
    "    df_doc_topic = pd.DataFrame(\n",
    "        m_doc_topic, \n",
    "        columns=[f\"topic_{i}\" for i in range(nmf.n_components)])\n",
    "\n",
    "    df_topic_word = pd.DataFrame(\n",
    "        m_topic_word, \n",
    "        columns=vocab)\n",
    "    \n",
    "    return {\"model\": nmf,\n",
    "            \"df_doc_topic\": df_doc_topic, \n",
    "            \"df_topic_word\": df_topic_word}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b6cd0-230a-4903-b0dd-89c41d7e82c0",
   "metadata": {},
   "source": [
    "#### BERTopic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99375706-5998-4e35-948d-80dcb0e6fd27",
   "metadata": {
    "gather": {
     "logged": 1744996835432
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def bertopic_fun(docs, number_topics, seed):\n",
    "    '''\n",
    "    Fits a BERTopic model and returns model, document-topic and topic-word matrices\n",
    "\n",
    "    Parameters:\n",
    "    - docs: list of str, input documents\n",
    "    - number_topics: int, number of topics\n",
    "    - seed: int, random seed for reproducibility\n",
    "\n",
    "    Outputs:\n",
    "    - bert: fitted BERTopic model\n",
    "    - df_doc_topic: pd.DataFrame, document-topic distribution (n_docs x n_topics)\n",
    "    - df_topic_word: pd.DataFrame, topic-word distribution (n_topics x words)\n",
    "\n",
    "    Notes:\n",
    "    - Output matrices are not yet normalized (row sums ≠ 1)\n",
    "    '''\n",
    "    # Define model\n",
    "    custom_vectorizer = CountVectorizer(\n",
    "        stop_words=\"english\", \n",
    "        lowercase=True, \n",
    "        max_df=0.5, \n",
    "        max_features=1000)\n",
    "    \n",
    "    umap_model = UMAP( \n",
    "        n_neighbors=15,  \n",
    "        n_components=5, # increases number of dim in dimenson reduction\n",
    "        min_dist=0.1, \n",
    "        metric='cosine',\n",
    "        random_state=seed) \n",
    "    \n",
    "    kmeans_model = KMeans(\n",
    "        n_clusters=number_topics)\n",
    "    \n",
    "    # Create BERTopic model\n",
    "    bert = BERTopic(\n",
    "        # embedding_model=finbert,\n",
    "        vectorizer_model=custom_vectorizer,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=kmeans_model,  #yYes, pass KMeans here!\n",
    "        verbose=True)\n",
    "    \n",
    "    # Fit the model\n",
    "    topics = bert.fit_transform(docs)\n",
    "\n",
    "    # Get topic-word-matrix. \n",
    "    # Note: Approximates document-topic distribution \n",
    "    m_doc_topic, _ = bert.approximate_distribution(docs) \n",
    "    df_doc_topic = pd.DataFrame(\n",
    "        m_doc_topic,\n",
    "        columns=[f\"topic_{i}\" for i in range(m_doc_topic.shape[1])])\n",
    "\n",
    "    # Topic-word matrix\n",
    "    words = bert.vectorizer_model.get_feature_names_out()\n",
    "    df_topic_word = pd.DataFrame(\n",
    "        bert.c_tf_idf_.todense(),\n",
    "        columns=words) \n",
    "    return {\"model\": bert,\n",
    "            \"df_doc_topic\": df_doc_topic, \n",
    "            \"df_topic_word\": df_topic_word}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c06a3-24d7-4ce3-b995-cb9cbf01d6d2",
   "metadata": {},
   "source": [
    "### Functions for evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c979455-cbc9-4f40-8be7-cd762147974e",
   "metadata": {},
   "source": [
    "#### umass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7b379-0428-4d2f-bb49-405ab7ad29b2",
   "metadata": {},
   "source": [
    "Note: The umass coherence score compares the co-occurrence probability of word pairs in the same topic with the probability of these word pairs occurring by chance. The score usually ranges from negative values to zero; numbers closer to zero mean better coherence/ more meaningful topics. Generally, scores around -1 or higher (closer to zero) are considered pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a3257-71da-4650-8a96-2ccf7c4688d2",
   "metadata": {
    "gather": {
     "logged": 1744996835545
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def compute_umass(df_topic_word, dtm, vocab, number_top_words=5):\n",
    "    \"\"\"\n",
    "    Compute mean u_mass coherence using metric_coherence_gensim.\n",
    "    Parameters:\n",
    "    - m_topic_word : pd.DataFrame, topic-word matrix\n",
    "    - dtm : np.ndarray, document-term matrix.\n",
    "    - vocab: list of str, feature names/ dtm column names (i.e., the words)\n",
    "    - number_top_words : int, Number of top words per topic.\n",
    "\n",
    "    Oputput:\n",
    "    - umass: float, mean umass coherence score\n",
    "    - does not require normalized topic-word matrix\n",
    "    \"\"\"\n",
    "    scores = metric_coherence_gensim(\n",
    "        measure=\"u_mass\",\n",
    "        top_n=number_top_words,\n",
    "        topic_word_distrib=df_topic_word.to_numpy(),\n",
    "        dtm=dtm,\n",
    "        vocab=vocab,\n",
    "        texts=None)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac54bc9-bd0a-44ca-8ef4-3e11ecbf4b62",
   "metadata": {},
   "source": [
    "#### Topic diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e970e5c-0c68-47c2-8d8e-5fa946452fa2",
   "metadata": {},
   "source": [
    "Note: Topic Diversity measures the ratio of unique top words across all topics to the total number of top words. Scores range from 0 to 1; values closer to 1 indicate that topics share fewer words (i.e. are more distinct and diverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f46fb-b833-4fc7-886f-5d7b43f84af4",
   "metadata": {
    "gather": {
     "logged": 1744996835616
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def compute_topic_diversity(df_topic_word, number_top_words=5):\n",
    "    \"\"\"\n",
    "    Compute topic diversity\n",
    " \n",
    "    Parameters:\n",
    "    - df_topic_word : pd.DataFrame, topic-word matrix\n",
    "    - number_top_words : int, number of top words per topic.\n",
    "\n",
    "    Output:\n",
    "    - topic diversity: float, topic diversity score\n",
    "\n",
    "    Notes:\n",
    "    - topic diversity = (num. of unique top words across topics) / (total num. of top words)\n",
    "    - does not require normalized topic-word matrix\n",
    "    \"\"\"\n",
    "    # use helper fun to get top words for each topic as list of list\n",
    "    top_words = get_topn_words(df_topic_word, number_top_words) \n",
    "\n",
    "    # Flatten all top words across topics and make list unique\n",
    "    top_words_flattend = []\n",
    "    for topic in top_words:         \n",
    "        for word in topic:          \n",
    "            top_words_flattend.append(word)\n",
    "    \n",
    "    top_words_unique = set(top_words_flattend)  \n",
    "\n",
    "    # calc and return topic diversity\n",
    "    return len(top_words_unique) / (len(top_words) * number_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d35af-2289-4fa1-9e0d-40afe6a00f9b",
   "metadata": {},
   "source": [
    "#### Intruder analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc66b4-07a7-4910-87cd-64fbe9bd178a",
   "metadata": {
    "gather": {
     "logged": 1744996835707
    }
   },
   "outputs": [],
   "source": [
    "# Define the functions\n",
    "def intruder_analysis(df_topic_word, vocab, number_top_words=5, seed=1):\n",
    "    \"\"\"\n",
    "    Generate the top words and a random intruder word for each topic in a string\n",
    "\n",
    "    Parameters:\n",
    "    - df_topic_word : pd.DataFrame, topic-word matrix\n",
    "    - vocab: list of str, feature names/ dtm column names (i.e., the words)\n",
    "    - number_top_words : int, Number of top words per topic\n",
    "    - seed: int, random seed for reproducibility\n",
    "\n",
    "    Ouptput:\n",
    "    - str, formatted topic + intruder lines for each topic    \n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Use helper function to get top words per topic\n",
    "    top_words = get_topn_words(df_topic_word, number_top_words)\n",
    "\n",
    "    # Flatten all top words across topics and make list unique\n",
    "    top_words_flattend = []\n",
    "    for topic in top_words:\n",
    "        for word in topic:\n",
    "            top_words_flattend.append(word)\n",
    "\n",
    "    top_words_unique = set(top_words_flattend)\n",
    "\n",
    "    # collect results as strings\n",
    "    output_lines = []\n",
    "\n",
    "    for i, topic_words in enumerate(top_words):\n",
    "        intr_candidates = [w for w in vocab if w not in topic_words and w not in top_words_unique]\n",
    "        intr_word = random.choice(intr_candidates) if intr_candidates else None\n",
    "        line = f\"Topic {i} words: {'|'.join(topic_words)} | Intruder: {intr_word}\"\n",
    "        output_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(output_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b923ee-fb06-46fa-9476-fea5a8c87519",
   "metadata": {},
   "source": [
    "### Calculate and display results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8fd7f-a0c7-4cbf-9163-9649e302bf85",
   "metadata": {},
   "source": [
    "#### Loop (incl. possibility for tuning topic numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4eef3-d249-4df1-8245-35c334e4ac4b",
   "metadata": {},
   "source": [
    "Implementation notes: (1)  dtm_tf and dtm_tfidf are unequal while vocab_tf and vocab_tfidf are equal. (2) For computing umass, one should use the same raw count-based DTM (dtm_tf) incl. the corresponding vocabulary vocab_tf for LDA, NMF, and BERTopic. (3) For intruder analysis, one should use the same vorabluary (vocab_tf) for LDA, NMF, and BERTopic. (4) In lda_fun and mnf_fun function calls we use vobab_tf and vocab_tfidf (eventhough they are identical) because of consistency with the parameters dtm_tf and dtm_tfidf (which are not identical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57e27e-cede-405a-80cb-d7d5578b3685",
   "metadata": {
    "gather": {
     "logged": 1744996835801
    }
   },
   "outputs": [],
   "source": [
    "# Specify number of topics for tuning\n",
    "L_number_topics = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941ab64-a653-46d5-92a1-a45e5f85b2f3",
   "metadata": {
    "gather": {
     "logged": 1744996835875
    }
   },
   "outputs": [],
   "source": [
    "# Prep data for BERTopic as list\n",
    "docs = df[\"text_clean\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e11f8-8d4c-4006-96f5-12f4dde5cfea",
   "metadata": {
    "gather": {
     "logged": 1744996835947
    }
   },
   "outputs": [],
   "source": [
    "# Loop: Fit models and get quantiative and qualitative measures\n",
    "dict_results = {}\n",
    "\n",
    "for number_topics in L_number_topics:\n",
    "\n",
    "    print(\"number_topics:\", number_topics)\n",
    "    \n",
    "    # LDA\n",
    "    out_lda = lda_fun(dtm=dtm_tf, vocab=vocab_tf, number_topics=number_topics, seed=seed)\n",
    "    umass_lda = compute_umass(df_topic_word=out_lda[\"df_topic_word\"], dtm=dtm_tf, vocab=vocab_tf, number_top_words=5)\n",
    "    topic_div_lda = compute_topic_diversity(df_topic_word=out_lda[\"df_topic_word\"], number_top_words=5)\n",
    "    intruder_lda = intruder_analysis(df_topic_word=out_lda[\"df_topic_word\"], vocab=vocab_tf, number_top_words=5, seed=seed)\n",
    "    dict_results[f\"LDA_{number_topics}\"] = {\"df_doc_topic\": out_lda[\"df_doc_topic\"], \"df_topic_word\": out_lda[\"df_topic_word\"], \"umass\": umass_lda, \"topic_diversity\": topic_div_lda, \"intruder\": intruder_lda}\n",
    "\n",
    "    # NMF\n",
    "    # out_nmf = nmf_fun(dtm=dtm_tfidf, vocab=vocab_tfidf, number_topics=number_topics)\n",
    "    # umass_nmf = compute_umass(df_topic_word=out_nmf[\"df_topic_word\"], dtm=dtm_tf, vocab=vocab_tf, number_top_words=5)\n",
    "    # topic_div_nmf = compute_topic_diversity(df_topic_word=out_nmf[\"df_topic_word\"], number_top_words=5)\n",
    "    # intruder_nmf = intruder_analysis(df_topic_word=out_nmf[\"df_topic_word\"], vocab=vocab_tf, number_top_words=5, seed=seed)\n",
    "    # dict_results[f\"NMF_{number_topics}\"] = {\"df_doc_topic\": out_nmf[\"df_doc_topic\"], \"df_topic_word\": out_nmf[\"df_topic_word\"], \"umass\": umass_nmf, \"topic_diversity\": topic_div_nmf, \"intruder\": intruder_nmf}\n",
    "\n",
    "    # # BERTopic\n",
    "    # out_bert = bertopic_fun(docs=docs, number_topics=number_topics, seed=seed)\n",
    "    # umass_bert = compute_umass(df_topic_word=out_bert[\"df_topic_word\"], dtm=dtm_tf, vocab=vocab_tf, number_top_words=5)\n",
    "    # topic_div_bert = compute_topic_diversity(df_topic_word=out_bert[\"df_topic_word\"], number_top_words=5)\n",
    "    # intruder_bert = intruder_analysis(df_topic_word=out_bert[\"df_topic_word\"], vocab=vocab_tf, number_top_words=5, seed=seed)\n",
    "    # dict_results[f\"BERTopic_{number_topics}\"] = {\"df_doc_topic\": out_bert[\"df_doc_topic\"], \"df_topic_word\": out_bert[\"df_topic_word\"], \"umass\": umass_bert, \"topic_diversity\": topic_div_bert, \"intruder\": intruder_bert}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81888e01-4d12-43b5-a8b7-97e89707bc7e",
   "metadata": {},
   "source": [
    "#### Quantiative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb39ea9f-dbba-417c-a20f-065e2984cea8",
   "metadata": {
    "gather": {
     "logged": 1744996836062
    }
   },
   "outputs": [],
   "source": [
    "# Create summary table from dict_results\n",
    "summary_rows = []\n",
    "\n",
    "for label, result in dict_results.items():\n",
    "    summary_rows.append({\n",
    "        \"model_number_topics\": label,\n",
    "        \"umass\": result[\"umass\"],\n",
    "        \"topic_diversity\": result[\"topic_diversity\"]})\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503e96b-0ee9-4a3d-a0ea-899d806da31d",
   "metadata": {},
   "source": [
    "#### Qualitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4678e9-eba4-43f0-9f2d-a0f4133d02ee",
   "metadata": {
    "gather": {
     "logged": 1744996836138
    }
   },
   "outputs": [],
   "source": [
    "# print the intruder analysis from dict_results\n",
    "for label, result in dict_results.items():\n",
    "    print(f\"{label} intruder analysis\")\n",
    "    print(result[\"intruder\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bad2c-4a4f-4be1-b0f6-d456e8b950d0",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73f1d9-4006-41d3-bb24-96773f0e59fe",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Note: In this section the document-topic and topic-word matrices are saved as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108864f0-3da3-4887-bb4e-c8fe4300f0bc",
   "metadata": {
    "gather": {
     "logged": 1744996836204
    }
   },
   "outputs": [],
   "source": [
    "# Ensure metadata is clean and aligned\n",
    "df_valid = df[df[\"text_clean_lemma\"].notna()].reset_index(drop=True)\n",
    "df_metadata = df_valid.drop(columns=[\"text\", \"text_clean\", \"text_clean_lemma\"]).reset_index(drop=True)\n",
    "\n",
    "# Save normalized matrices with consistent and clear naming\n",
    "for model_name, result in dict_results.items():\n",
    "    # Normalize both matrices\n",
    "    df_doc_topic_norm = normalize_matrix(result[\"df_doc_topic\"]).reset_index(drop=True)\n",
    "    df_topic_word_norm = normalize_matrix(result[\"df_topic_word\"])\n",
    "\n",
    "    # Augment the document-topic matrix with metadata\n",
    "    df_doc_topic_norm_augmented = pd.concat([df_metadata, df_doc_topic_norm], axis=1)\n",
    "\n",
    "    # Save both matrices\n",
    "    df_doc_topic_norm_augmented.to_csv(f\"{model_name}_doc_topic_norm_augmented.csv\", index=False)\n",
    "    df_topic_word_norm.to_csv(f\"{model_name}_topic_word_norm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348670e-5022-439f-94dc-a1d8bad75ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "clean38"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
